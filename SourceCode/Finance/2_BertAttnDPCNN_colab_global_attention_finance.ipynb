{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "1_BertAttnDPCNN5_colab_attention_False_finance Exp7.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.2"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "# Adapted from:\r\n",
        "#\\https://www.analyticsvidhya.com/blog/2020/07/transfer-learning-for-nlp-fine-tuning-bert-for-text-classification/\r\n",
        "#https://www.programmersought.com/article/31037125394/"
      ],
      "outputs": [],
      "metadata": {
        "id": "8fghEE8Hnf9L"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "pip install transformers"
      ],
      "outputs": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XpYzR-xRn1pY",
        "outputId": "aa2205ef-7964-4209-dcdf-c08f147de466"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "#from google.colab import drive\r\n",
        "#drive.mount('/content/drive')"
      ],
      "outputs": [],
      "metadata": {
        "id": "zDP0BdP7ZsXd"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "import torch\r\n",
        "import torch.nn as nn\r\n",
        "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\r\n",
        "import random\r\n",
        "import numpy as np\r\n",
        "import pandas as pd\r\n",
        "import torch.nn.functional as F\r\n",
        "from sklearn.model_selection import train_test_split\r\n",
        "from sklearn.metrics import classification_report\r\n",
        "import transformers\r\n",
        "from transformers import AutoModel, BertTokenizerFast\r\n",
        "import time\r\n",
        "from sklearn.metrics import classification_report\r\n",
        "SEED = 1234\r\n",
        "\r\n",
        "random.seed(SEED)\r\n",
        "np.random.seed(SEED)\r\n",
        "torch.manual_seed(SEED)\r\n",
        "torch.backends.cudnn.deterministic = True\r\n",
        "\r\n",
        "# specify GPU\r\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ],
      "outputs": [],
      "metadata": {
        "id": "IUAEUPTPnf9Q"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "outputs": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DSQwK5wE6Cp7",
        "outputId": "ababbcc6-9b7d-4164-c7f0-8c0feb0cfc24"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "#df_all = pd.read_csv('all-data.csv',encoding='latin-1')\n",
        "df_all = pd.read_csv('/content/drive/My Drive/content/all-data.csv', encoding = 'latin-1', header=None )\n",
        "df_all.columns = [\"sentiment\", \"text\"]"
      ],
      "outputs": [],
      "metadata": {
        "id": "ydDYRF92aDSk"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "df_all.head()"
      ],
      "outputs": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 203
        },
        "id": "D_SK46UW-hC-",
        "outputId": "ee4e3ab8-73e9-4886-d833-fc81b090ce9e"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "#df_all = df_all[['text','sentiment']]\n",
        "dic = {'negative' : 0, 'neutral': 1, 'positive': 2}\n",
        "df_all['sentiment']= df_all['sentiment'].replace(dic).astype('int64')\n",
        "df_all.head()"
      ],
      "outputs": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 203
        },
        "id": "RdbPTwVlaTck",
        "outputId": "a208416e-fe58-416b-bacf-5cd7a605cd1a"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "# split train dataset into train, validation and test sets\n",
        "train_text, temp_text, train_labels, temp_labels = train_test_split(df_all['text'], df_all['sentiment'], \n",
        "                                                                    random_state=45, \n",
        "                                                                    test_size=0.3, \n",
        "                                                                    stratify=df_all['sentiment'])\n",
        "\n",
        "\n",
        "val_text, test_text, val_labels, test_labels = train_test_split(temp_text, temp_labels, \n",
        "                                                                random_state=45, \n",
        "                                                                test_size=0.5, \n",
        "                                                                stratify=temp_labels)"
      ],
      "outputs": [],
      "metadata": {
        "id": "ISP1tMtncuGq"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "from transformers import AutoModel, BertTokenizerFast\n",
        "# import BERT-base pretrained model\n",
        "bert = AutoModel.from_pretrained('bert-base-uncased')\n",
        "\n",
        "# Load the BERT tokenizer\n",
        "tokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased')"
      ],
      "outputs": [],
      "metadata": {
        "id": "-2A3AcBSd13Y"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "# freeze all the parameters\n",
        "for param in bert.parameters():\n",
        "    param.requires_grad = False"
      ],
      "outputs": [],
      "metadata": {
        "id": "9h6xYyUggJPv"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "# get length of all the messages in the train set\n",
        "seq_len = [len(i.split()) for i in train_text]\n",
        "pd.Series(seq_len).hist(bins = 30)"
      ],
      "outputs": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 283
        },
        "id": "27n6jQ7Ad0MZ",
        "outputId": "6166a457-078b-445b-b7cf-b6b49ba0a958"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "# get length of all the messages in the val set\n",
        "seq_len = [len(i.split()) for i in val_text]\n",
        "pd.Series(seq_len).hist(bins = 30)"
      ],
      "outputs": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 283
        },
        "id": "ThFIrJ1_eRHY",
        "outputId": "41514668-2b2d-4181-e245-309d2c516b96"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "# get length of all the messages in the test set\n",
        "seq_len = [len(i.split()) for i in test_text]\n",
        "pd.Series(seq_len).hist(bins = 30)"
      ],
      "outputs": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 283
        },
        "id": "6lDpMvoJeSDY",
        "outputId": "91aff5c0-9e68-4add-cd1d-6d76486a41d8"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "# tokenize and encode sequences in the training set\n",
        "tokens_train = tokenizer.batch_encode_plus(\n",
        "    train_text.tolist(),\n",
        "    max_length = 40,\n",
        "    padding='longest',\n",
        "    truncation=True\n",
        ")\n",
        "\n",
        "# tokenize and encode sequences in the validation set\n",
        "tokens_val = tokenizer.batch_encode_plus(\n",
        "    val_text.tolist(),\n",
        "    max_length = 40,\n",
        "    padding='longest',\n",
        "    truncation=True\n",
        ")\n",
        "\n",
        "# tokenize and encode sequences in the test set\n",
        "tokens_test = tokenizer.batch_encode_plus(\n",
        "    test_text.tolist(),\n",
        "    max_length = 40,\n",
        "    padding='longest',\n",
        "    truncation=True\n",
        ")"
      ],
      "outputs": [],
      "metadata": {
        "id": "k4G9gD3neNkJ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "## convert lists to tensors\n",
        "\n",
        "train_seq = torch.tensor(tokens_train['input_ids'])\n",
        "train_mask = torch.tensor(tokens_train['attention_mask'])\n",
        "train_y = torch.tensor(train_labels.tolist())\n",
        "\n",
        "val_seq = torch.tensor(tokens_val['input_ids'])\n",
        "val_mask = torch.tensor(tokens_val['attention_mask'])\n",
        "val_y = torch.tensor(val_labels.tolist())\n",
        "\n",
        "test_seq = torch.tensor(tokens_test['input_ids'])\n",
        "test_mask = torch.tensor(tokens_test['attention_mask'])\n",
        "test_y = torch.tensor(test_labels.tolist())"
      ],
      "outputs": [],
      "metadata": {
        "id": "ChuoX_0XfU4_"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "#define a batch size\n",
        "batch_size = 32\n",
        "\n",
        "# wrap tensors\n",
        "train_data = TensorDataset(train_seq, train_mask, train_y)\n",
        "\n",
        "# sampler for sampling the data during training\n",
        "train_sampler = RandomSampler(train_data)\n",
        "\n",
        "# dataLoader for train set\n",
        "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size, drop_last=True)\n",
        "\n",
        "# wrap tensors\n",
        "val_data = TensorDataset(val_seq, val_mask, val_y)\n",
        "\n",
        "# sampler for sampling the data during training\n",
        "val_sampler = SequentialSampler(val_data)\n",
        "\n",
        "# dataLoader for validation set\n",
        "val_dataloader = DataLoader(val_data, sampler = val_sampler, batch_size=batch_size, drop_last=True)\n",
        "\n",
        "# wrap tensors\n",
        "test_data = TensorDataset(test_seq, test_mask, test_y)\n",
        "\n",
        "# sampler for sampling the data during training\n",
        "test_sampler = SequentialSampler(test_data)\n",
        "\n",
        "# dataLoader for test set\n",
        "test_dataloader = DataLoader(test_data, sampler = test_sampler, batch_size=batch_size, drop_last=True)"
      ],
      "outputs": [],
      "metadata": {
        "id": "9oyz5bTJfVnf"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "\n",
        "#print('train loader',len(train_dataloader.dataset))\n",
        "#print('val loader',len(val_dataloader.dataset))\n",
        "#print('test loader',len(test_loader))"
      ],
      "outputs": [],
      "metadata": {
        "id": "MflsFK7RANpl"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "#import torch.nn as nn\n",
        "#import torch.nn.functional as F\n",
        "from torch.autograd import Variable\n",
        "\n",
        "\n",
        "class AttnDPCNN(nn.Module):\n",
        "\n",
        "    def __init__(self, batch_size,seq_len,embedding_dim,output_size_linear, \n",
        "                 cnn_channel_size, cnn_kernel_size=(3,1),cnn_padding=0, cnn_stride=1, attn_nodes_per_layer = 10):\n",
        "\n",
        "        super(AttnDPCNN, self).__init__()\n",
        "        #self.bert=bert\n",
        "        self.attn_nodes_per_layer = attn_nodes_per_layer\n",
        "        self.channel_size = cnn_channel_size\n",
        "        self.output_size_linear = output_size_linear\n",
        "        self.cnn_kernel_size = cnn_kernel_size\n",
        "        self.padding = cnn_padding\n",
        "        self.stride = cnn_stride\n",
        "        #self.vocab_size = vocab_size\n",
        "        self.embed_dim = embedding_dim\n",
        "        self.seq_len = seq_len\n",
        "        self.batch_size = batch_size\n",
        "        \n",
        "        #self.collect_out_softmax = torch.zeros(1,1).to(device)\n",
        "        #self.collect_out_attn = torch.zeros(self.batch_size,self.attn_nodes_per_layer).to(device)\n",
        "        \n",
        "        #self.embed = nn.Embedding(self.vocab_size, self.embed_dim)\n",
        "        self.conv1 = nn.Conv2d(1,  self.channel_size, (3,self.embed_dim), stride = 1) # embed_dim becomes 1\n",
        "        self.conv2 = nn.Conv2d(self.channel_size,  self.channel_size,  \n",
        "                                kernel_size=self.cnn_kernel_size, padding=0,\n",
        "                               stride = self.stride)\n",
        "        self.padding1 = nn.ZeroPad2d((0, 0, 1, 1))  # top bottom\n",
        "        self.padding2 = nn.ZeroPad2d((0, 0, 0, 1))  # bottom\n",
        "        self.max_pool = nn.MaxPool2d(kernel_size=(3, 1), stride=2)      \n",
        "        \n",
        "        self._repeat_block = nn.Sequential( # input [batch_size, num_filters, seq_len-3+1, 1]\n",
        "                    nn.ZeroPad2d((0, 0, 1, 1)), # [batch_size, num_filters, seq_len, 1]\n",
        "                    nn.ReLU(),\n",
        "                    nn.Conv2d(self.channel_size,  self.channel_size,   # [batch_size, num_filters, seq_len-3+1, 1]\n",
        "                                kernel_size=self.cnn_kernel_size, padding=0,\n",
        "                               stride = self.stride),\n",
        "                    nn.ZeroPad2d((0, 0, 1, 1)), # [batch_size, num_filters, seq_len, 1]\n",
        "                    nn.ReLU(),\n",
        "                    nn.Conv2d(self.channel_size,  self.channel_size,  # [batch_size, num_filters, seq_len-3+1, 1]\n",
        "                                kernel_size=self.cnn_kernel_size, padding=0,\n",
        "                               stride = self.stride),\n",
        "            )\n",
        "        \n",
        "        \n",
        "        # To get variying attention weights for 1) softmax output 2) 10 representing features\n",
        "        self.num_repeat_blocks, self.size_repeat_blocks_list = self.dividible(2)\n",
        "        self.weight_softmax_list, self.weight_attn_list = self.getWeights_init()\n",
        "        \n",
        "        self.counter=0\n",
        "        \n",
        "        # Drop probability\n",
        "        self.dropout = nn.Dropout(p=0.2)\n",
        "        # Last linear layer (FC)\n",
        "        self.linear1 = nn.Linear(self.attn_nodes_per_layer*self.num_repeat_blocks,self.output_size_linear)\n",
        "        #self.linear1 = nn.Linear(self.channel_size,self.output_size_linear)\n",
        "          \n",
        "    def repeat_block(self, x): # input [batch_size, num_filters, seq_len-3+1, 1]\n",
        "        x = self.padding2(x) # [batch_size, num_filters, seq_len-1, 1] -> seq_len-3+1+1 = seq_len-1\n",
        "        #print('padding')\n",
        "        x1 = self.max_pool(x)  # [batch_size, num_filters, (seq_len-1) // 3, 1 //1] -> seq_len-3+1+1 = seq_len-1\n",
        "        #print('x1',x1.shape)\n",
        "        # conv block\n",
        "        x2 = self._repeat_block(x1)\n",
        "        #print('x2',x2.shape)\n",
        "        # Identity addition\n",
        "        x = x1 + x2\n",
        "        #print('x1+x2',x.shape)\n",
        "        # to get attn score\n",
        "        #print(\"out softmax weights\",self.weight_softmax_list[self.counter].shape)\n",
        "        #out_softmax = F.linear(x.reshape(1,-1),self.weight_softmax_list[self.counter])\n",
        "        \n",
        "        # fix output features as 10\n",
        "        out_attn = F.linear(x.reshape(x.shape[0],-1),self.weight_attn_list[self.counter])\n",
        "\n",
        "        return x,out_attn\n",
        "\n",
        "    def forward(self, x):\n",
        "        \n",
        "        self.counter =0\n",
        "\n",
        "        #embed = self.bert(sent_id, attention_mask=mask)\n",
        "        #x = embed['last_hidden_state'].unsqueeze(1) # [batch_size, 1, seq_len, embedding_dim]\n",
        "        x = self.conv1(x)  # [batch_size, num_filters, seq_len-3+1, 1]\n",
        "        #print(x.shape)\n",
        "        x = self.padding1(x)  # [batch_size, num_filters, seq_len, 1]\n",
        "        x = F.relu(x)\n",
        "        x = self.conv2(x)  # [batch_size, num_filters, seq_len-3+1, 1]\n",
        "\n",
        "        while x.size()[2] >= 2: # till seq_len is 1\n",
        "            #print(x.shape)\n",
        "            x,out_attn = self.repeat_block(x)  # [batch_size, num_filters,1,1] at the end\n",
        "            #print(out_attn.shape) # 32 x 10\n",
        "            \n",
        "            if self.counter==0:\n",
        "                #self.collect_out_softmax = out_softmax\n",
        "                self.collect_out_attn = out_attn.unsqueeze(2)\n",
        "                #self.collect_out_attn = [out_attn]\n",
        "            else:\n",
        "                #self.collect_out_softmax = torch.cat((self.collect_out_softmax,out_softmax),0)\n",
        "                self.collect_out_attn = torch.cat((self.collect_out_attn,out_attn.unsqueeze(2)),2) \n",
        "                #self.collect_out_attn.append(out_attn)\n",
        "        \n",
        "            self.counter +=1\n",
        "\n",
        "\n",
        "        ## CALCULATION OF DOT PRODUCT ATTENTION ##\n",
        "        #print(\"collect out attn\",self.collect_out_attn.shape) # 32 x 10 x 5\n",
        "        final_collect_out_attn = torch.zeros_like(self.collect_out_attn)\n",
        "        key = self.collect_out_attn.clone() # 32 x 10 x 5\n",
        "        value = self.collect_out_attn.clone() # 32 x 10 x 5\n",
        "        for i in range(self.counter):\n",
        "\n",
        "            #start = i*self.attn_nodes_per_layer\n",
        "            #end = start+self.attn_nodes_per_layer\n",
        "            query = self.collect_out_attn[:,:,i].clone().unsqueeze(2) # 32 x 10 x 1\n",
        "\n",
        "            # dot product and then softmax\n",
        "            in_softmax = torch.bmm(key.transpose(1,2),query)/self.num_repeat_blocks\n",
        "            #print(\"in softmax\",in_softmax.shape) #32 x 5 x 1\n",
        "            #print(\"in soft scores sum\",in_softmax[0,:,0])\n",
        "            prob = F.softmax(in_softmax,1)\n",
        "            #print(\"prob\",prob.shape) #32x 5 x 1\n",
        "            #print(\"prob sum\",prob[0,:,0])\n",
        "            \n",
        "            final_vec = torch.bmm(value,prob) # 32 x 10 x 1\n",
        "            #print(\"final_vec\",final_vec.shape)\n",
        "\n",
        "            if i==0:\n",
        "              output = final_vec\n",
        "              maps = prob.detach().cpu()\n",
        "            else:\n",
        "              output = torch.cat((output,final_vec),2)\n",
        "              maps = torch.cat((maps,prob.detach().cpu()),2)\n",
        "        \n",
        "        #output =  output/self.num_repeat_blocks\n",
        "        #print(\"output\",output.shape) # 32 x 10 x 5\n",
        "\n",
        "\n",
        "\n",
        "        ## FULLY CONNECTED ##\n",
        "        # linear layer\n",
        "        output = self.linear1(output.reshape(x.shape[0],-1))  # [batch_size, output size]\n",
        "        output = self.dropout(output)\n",
        "        return output, maps\n",
        "    \n",
        "    def dividible(self,p):\n",
        "        n=self.seq_len\n",
        "        i = 0\n",
        "        out=[]\n",
        "        while n>=p:\n",
        "            if i==0:\n",
        "                n = (n-2)//2\n",
        "               #n=n-2\n",
        "               #out.append(n-1)\n",
        "            else:\n",
        "                n = n//2\n",
        "                #out.append(n)\n",
        "            i += 1\n",
        "            out.append(n)\n",
        "        return i,out\n",
        "    \n",
        "    def getWeights_init(self):\n",
        "        weights_softmax_list = []\n",
        "        weights_attn_list = []\n",
        "        for i in range(self.num_repeat_blocks):\n",
        "            # size_repeat_blocks_list contains various seq length\n",
        "            #print('batch size', self.batch_size)\n",
        "            #print('seq len', self.size_repeat_blocks_list[i])\n",
        "            #print('channel size', self.channel_size)\n",
        "            weights_softmax_list.append(Variable(torch.randn(1,self.batch_size*1*self.size_repeat_blocks_list[i]*self.channel_size)).to(device))\n",
        "            weights_attn_list.append(Variable(torch.randn(self.attn_nodes_per_layer,1*self.size_repeat_blocks_list[i]*self.channel_size)).to(device)) # fix output features as 10\n",
        "        return weights_softmax_list,weights_attn_list\n",
        "            "
      ],
      "outputs": [],
      "metadata": {
        "id": "myQdGVCufxZe"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "dpcnn = AttnDPCNN(32,40,768, 3, 128) # batchsize, seq len, embed dim, classes, no of filters\n",
        "dpcnn = dpcnn.to(device)\n",
        "bert = bert.to(device)"
      ],
      "outputs": [],
      "metadata": {
        "id": "7Sz1FnUIgDb_"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "# optimizer from hugging face transformers\n",
        "from transformers import AdamW\n",
        "\n",
        "# define the optimizer\n",
        "#optimizer_bert = AdamW(bert.parameters(), lr = 1e-5)          # lr=learning rate\n",
        "optimizer_dpcnn = AdamW(dpcnn.parameters(), lr = 0.001)\n",
        "#scheduler_bert = torch.optim.lr_scheduler.StepLR(optimizer_bert, step_size = 2, gamma=0.8)\n",
        "scheduler_dpcnn = torch.optim.lr_scheduler.StepLR(optimizer_dpcnn, step_size = 3, gamma=0.1)"
      ],
      "outputs": [],
      "metadata": {
        "id": "CXRr52J0gDuf"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "\n",
        "#compute the class weights\n",
        "class_weights = compute_class_weight('balanced', np.unique(train_labels), train_labels)\n",
        "\n",
        "print(\"Class Weights:\",class_weights)"
      ],
      "outputs": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bpEEKpjWgD6P",
        "outputId": "1578b6a8-0426-409c-ca87-9cb0f0b1d536"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "# converting list of class weights to a tensor\n",
        "weights= torch.tensor(class_weights,dtype=torch.float)\n",
        "\n",
        "# push to GPU\n",
        "weights = weights.to(device)\n",
        "\n",
        "# define the loss function\n",
        "cross_entropy  = nn.CrossEntropyLoss(weight=weights) \n",
        "\n",
        "# number of training epochs\n",
        "epochs = 20"
      ],
      "outputs": [],
      "metadata": {
        "id": "Zd0F4yoKv9L8"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "# function to train the model\n",
        "def train():\n",
        "  \n",
        "  bert.train()\n",
        "  dpcnn.train()\n",
        "\n",
        "  total_loss, total_accuracy = 0, 0\n",
        "  \n",
        "  # empty list to save model predictions\n",
        "  total_preds=[]\n",
        "  \n",
        "  # iterate over batches\n",
        "  for step,batch in enumerate(train_dataloader):\n",
        "    \n",
        "    # progress update after every 50 batches.\n",
        "    if step % 50 == 0 and not step == 0:\n",
        "      print('  Batch {:>5,}  of  {:>5,}.'.format(step, len(train_dataloader)))\n",
        "\n",
        "    # push the batch to gpu\n",
        "    batch = [r.to(device) for r in batch]\n",
        " \n",
        "    sent_id, mask, labels = batch\n",
        "\n",
        "    # clear previously calculated gradients \n",
        "\n",
        "    #optimizer_bert.zero_grad()\n",
        "    optimizer_dpcnn.zero_grad()   \n",
        "\n",
        "    # get model predictions for the current batch\n",
        "    \n",
        "    embed = bert(sent_id, attention_mask=mask)\n",
        "    x = embed['last_hidden_state'].unsqueeze(1) # [batch_size, 1, seq_len, embedding_dim]\n",
        "    preds,_ = dpcnn(x)\n",
        "\n",
        "    # compute the loss between actual and predicted values\n",
        "    loss = cross_entropy(preds, labels)\n",
        "\n",
        "    # add on to the total loss\n",
        "    total_loss = total_loss + loss.item()\n",
        "\n",
        "    # backward pass to calculate the gradients\n",
        "    loss.backward()\n",
        "\n",
        "    # clip the the gradients to 1.0. It helps in preventing the exploding gradient problem\n",
        "    #torch.nn.utils.clip_grad_norm_(bert.parameters(), 1.0)\n",
        "    torch.nn.utils.clip_grad_norm_(dpcnn.parameters(), 1.0)\n",
        "  \n",
        "    # update parameters\n",
        "    #optimizer_bert.step()\n",
        "    optimizer_dpcnn.step() \n",
        "\n",
        "    # model predictions are stored on GPU. So, push it to CPU\n",
        "    preds=preds.detach().cpu().numpy()\n",
        "\n",
        "    # append the model predictions\n",
        "    total_preds.append(preds)\n",
        "    \n",
        "  # compute the training loss of the epoch\n",
        "  avg_loss = total_loss / len(train_dataloader)\n",
        "  \n",
        "  # predictions are in the form of (no. of batches, size of batch, no. of classes).\n",
        "  # reshape the predictions in form of (number of samples, no. of classes)\n",
        "  total_preds  = np.concatenate(total_preds, axis=0)\n",
        "\n",
        "  #returns the loss and predictions\n",
        "  return avg_loss, total_preds"
      ],
      "outputs": [],
      "metadata": {
        "id": "oo4LP4egv9xS"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "# function for evaluating the model\n",
        "def evaluate():\n",
        "  \n",
        "  print(\"\\nEvaluating...\")\n",
        "  t0 = time.time()\n",
        "  # deactivate dropout layers\n",
        "  #model.eval() replace below?\n",
        "  bert.eval()\n",
        "  dpcnn.eval()\n",
        "\n",
        "  total_loss, total_accuracy = 0, 0\n",
        "  \n",
        "  # empty list to save the model predictions\n",
        "  total_preds = []\n",
        "\n",
        "  # iterate over batches\n",
        "  for step,batch in enumerate(val_dataloader):\n",
        "    \n",
        "    # Progress update every 50 batches.\n",
        "    if step % 50 == 0 and not step == 0:\n",
        "      \n",
        "      # Calculate elapsed time in minutes.\n",
        "      elapsed = time.time() - t0\n",
        "            \n",
        "      # Report progress.\n",
        "      print('  Batch {:>5,}  of  {:>5,}.'.format(step, len(val_dataloader)))\n",
        "\n",
        "    # push the batch to gpu\n",
        "    batch = [t.to(device) for t in batch]\n",
        "\n",
        "    sent_id, mask, labels = batch\n",
        "\n",
        "    # deactivate autograd\n",
        "    with torch.no_grad():\n",
        "      \n",
        "      # model predictions\n",
        "      embed = bert(sent_id, attention_mask=mask)\n",
        "      x = embed['last_hidden_state'].unsqueeze(1) # [batch_size, 1, seq_len, embedding_dim]\n",
        "      preds,_ = dpcnn(x)\n",
        "\n",
        "      # compute the validation loss between actual and predicted values\n",
        "      loss = cross_entropy(preds,labels)\n",
        "\n",
        "      total_loss = total_loss + loss.item()\n",
        "\n",
        "      preds = preds.detach().cpu().numpy()\n",
        "\n",
        "      total_preds.append(preds)\n",
        "\n",
        "  # compute the validation loss of the epoch\n",
        "  avg_loss = total_loss / len(val_dataloader) \n",
        "\n",
        "  # reshape the predictions in form of (number of samples, no. of classes)\n",
        "  total_preds  = np.concatenate(total_preds, axis=0)\n",
        "\n",
        "  return avg_loss, total_preds"
      ],
      "outputs": [],
      "metadata": {
        "id": "1wMHjrhtv966"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "#print(model.num_repeat_blocks)\n",
        "#print(model.size_repeat_blocks_list)"
      ],
      "outputs": [],
      "metadata": {
        "id": "gNb5Sroq4Kt4"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "# set initial loss to infinite\n",
        "best_valid_loss = float('inf')\n",
        "\n",
        "# empty lists to store training and validation loss of each epoch\n",
        "train_losses=[]\n",
        "valid_losses=[]\n",
        "\n",
        "\n",
        "#for each epoch\n",
        "for epoch in range(1, epochs+1):\n",
        "\n",
        "    print('\\n Epoch {:} / {:}'.format(epoch, epochs))\n",
        "    \n",
        "    #train model\n",
        "    train_loss, _ = train()\n",
        "    \n",
        "    #evaluate model\n",
        "    valid_loss, _ = evaluate()\n",
        "\n",
        "    #scheduler_bert.step()\n",
        "    scheduler_dpcnn.step()\n",
        "    \n",
        "    #save the best model\n",
        "    #if valid_loss < best_valid_loss:\n",
        "    #    best_valid_loss = valid_loss\n",
        "    #    torch.save(bert.state_dict(), 'bert_saved_weights.pt')\n",
        "    #    torch.save(dpcnn.state_dict(), 'dpcnn_saved_weights.pt')\n",
        "    \n",
        "    # append training and validation loss\n",
        "    train_losses.append(train_loss)\n",
        "    valid_losses.append(valid_loss)\n",
        "  \n",
        "    print(f'\\nTraining Loss: {train_loss:.3f}')\n",
        "    print(f'Validation Loss: {valid_loss:.3f}')\n",
        "    #break"
      ],
      "outputs": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jMS0f0EK0uN-",
        "scrolled": true,
        "outputId": "75fa31e4-4b12-4301-9ea7-cb2e9d8f57d7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "TEST"
      ],
      "metadata": {
        "id": "35jFhznsFjeE"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "# function for evaluating the model\n",
        "def test():\n",
        "  \n",
        "  print(\"\\nEvaluating...\")\n",
        "  t0 = time.time()\n",
        "  # deactivate dropout layers\n",
        "  #model.eval() replace below?\n",
        "\n",
        "  #load weights of best model\n",
        "  #bpath = 'bert_saved_weights.pt'\n",
        "  #bert.load_state_dict(bert.load(bpath))\n",
        "\n",
        "  bert.eval()\n",
        "  dpcnn.eval()\n",
        "\n",
        "  total_loss, total_accuracy = 0, 0\n",
        "  \n",
        "  # empty list to save the model predictions\n",
        "  total_preds = []\n",
        "  total_targets = []\n",
        "  maps_list=[]\n",
        "  # iterate over batches\n",
        "  for step,batch in enumerate(test_dataloader):\n",
        "    \n",
        "    # Progress update every 50 batches.\n",
        "    if step % 50 == 0 and not step == 0:\n",
        "      \n",
        "      # Calculate elapsed time in minutes.\n",
        "      elapsed = time.time() - t0\n",
        "            \n",
        "      # Report progress.\n",
        "      print('  Batch {:>5,}  of  {:>5,}.'.format(step, len(test_dataloader)))\n",
        "\n",
        "    # push the batch to gpu\n",
        "    batch = [t.to(device) for t in batch]\n",
        "\n",
        "    sent_id, mask, labels = batch\n",
        "\n",
        "    # deactivate autograd\n",
        "    with torch.no_grad():\n",
        "      \n",
        "      # model predictions\n",
        "      embed = bert(sent_id, attention_mask=mask)\n",
        "      x = embed['last_hidden_state'].unsqueeze(1) # [batch_size, 1, seq_len, embedding_dim]\n",
        "      preds,maps = dpcnn(x)\n",
        "      maps_list.append(maps)\n",
        "      # compute the validation loss between actual and predicted values\n",
        "      loss = cross_entropy(preds,labels)\n",
        "\n",
        "      total_loss = total_loss + loss.item()\n",
        "\n",
        "      preds = preds.detach().cpu()\n",
        "      labels = labels.detach().cpu()\n",
        "\n",
        "      total_preds.append(preds.numpy())\n",
        "      total_targets.append(labels.numpy())\n",
        "  # compute the validation loss of the epoch\n",
        "  avg_loss = total_loss / len(test_dataloader) \n",
        "\n",
        "  # reshape the predictions in form of (number of samples, no. of classes)\n",
        "  total_preds  = np.concatenate(total_preds, axis=0)\n",
        "  total_targets = np.concatenate(total_targets, axis=0)\n",
        "  dpcnn.maps = maps_list\n",
        "  return avg_loss, total_preds, total_targets"
      ],
      "outputs": [],
      "metadata": {
        "id": "k4H1U-OH0u4u"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "#load weights of best model\n",
        "#bpath = 'bert_saved_weights.pt'\n",
        "#bert.load_state_dict(bert.load(bpath))\n",
        "#load weights of best model\n",
        "#dpath = 'dpcnn_saved_weights.pt'\n",
        "#dpcnn.load_state_dict(dpcnn.load(dpath))"
      ],
      "outputs": [],
      "metadata": {
        "id": "Lta77CwxFTzF"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "test_loss, test_preds, test_targets = test()\n",
        "print(f'Test Loss: {test_loss:.3f}')"
      ],
      "outputs": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g2KzqQVS4Aro",
        "outputId": "3b910bce-fe4c-461b-b8c5-7b34ef70609d"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "test_preds_classes = np.argmax(test_preds, axis = 1)\n",
        "print(classification_report(test_targets, test_preds_classes))"
      ],
      "outputs": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oGeUQF3F0viG",
        "outputId": "898039fe-0cff-45cb-b29b-c52927cb6638"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "# Get confusion matrix\n",
        "from sklearn.metrics import confusion_matrix\n",
        "cf = confusion_matrix(test_targets, test_preds_classes)\n",
        "cf #cf[actu][pred]\n",
        "# pred 0 1 2 3\n",
        "# actu\n",
        "# 0\n",
        "# 1\n",
        "# 2\n",
        "# 3"
      ],
      "outputs": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tyJMgmgJ0v_2",
        "outputId": "4dffcce2-093b-4db0-a65d-b1e392d9f728"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "import matplotlib.pyplot as plt # we only need pyplot\n",
        "import matplotlib.cm as cm\n",
        "import matplotlib\n",
        "\n",
        "# Get components of actual labels\n",
        "def getPredictionOfActualLabels(cf,actual_label,angle=45):\n",
        "    sizes = []\n",
        "    # cf[actual-1][predicted-1]\n",
        "    #pa1 = cf[actual_label-1][0]\n",
        "    #pa2 = cf[actual_label-1][1]\n",
        "    #pa3 = cf[actual_label-1][2]\n",
        "    #pa4 = cf[actual_label-1][3]\n",
        "    #pa5 = cf[actual_label-1][4]\n",
        "    \n",
        "    for i in range(3):\n",
        "        sizes.append(cf[actual_label][i])\n",
        "    \n",
        "    # Data to plot\n",
        "    labels = ['negative', 'neutral', 'positive']\n",
        "    #sizes = [p[0], p[1], p[2], p[3]]\n",
        "    colors = ['lightskyblue', 'yellowgreen', 'lightcoral']\n",
        "\n",
        "    # Plot\n",
        "    plt.pie(sizes, labels=labels, colors=colors, autopct='%1.0f%%', shadow=False, startangle=angle, textprops={'fontsize': 14})\n",
        "    plt.title(\"Predictions of Ratings with actual labels as \"+labels[actual_label],fontsize=20)\n",
        "    #matplotlib.rcParams['text.color'] = 'white'\n",
        "\n",
        "    plt.axis('equal')\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "  # Get components of predictions\n",
        "def getActualLabelsOfPredictions(cf,pred_label,angle=45):\n",
        "    sizes = []\n",
        "    # cf[actual-1][predicted-1]\n",
        "    #pa1 = cf[actual_label-1][0]\n",
        "    #pa2 = cf[actual_label-1][1]\n",
        "    #pa3 = cf[actual_label-1][2]\n",
        "    #pa4 = cf[actual_label-1][3]\n",
        "    #pa5 = cf[actual_label-1][4]\n",
        "    \n",
        "    for i in range(3):\n",
        "        sizes.append(cf[i][pred_label])\n",
        "    \n",
        "    # Data to plot\n",
        "    labels = ['negative', 'neutral', 'positive']\n",
        "    #sizes = [p[0], p[1], p[2], p[3]]\n",
        "    colors = ['lightskyblue', 'yellowgreen', 'lightcoral']\n",
        "\n",
        "    # Plot\n",
        "    plt.pie(sizes, labels=labels, colors=colors, autopct='%1.0f%%', shadow=False, startangle=angle, textprops={'fontsize': 14})\n",
        "    plt.title(\"Actual Labels of Ratings predicted as \"+labels[pred_label],fontsize=20)\n",
        "    #matplotlib.rcParams['text.color'] = 'white'\n",
        "\n",
        "    plt.axis('equal')\n",
        "    plt.show()"
      ],
      "outputs": [],
      "metadata": {
        "id": "u2vzhlfz0wM9"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "getPredictionOfActualLabels(cf,0,angle=45)\n"
      ],
      "outputs": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 269
        },
        "id": "il77JAIsv-C4",
        "outputId": "00b23227-294c-4515-9d95-b35b31afcdd5"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "getPredictionOfActualLabels(cf,1,angle=45)"
      ],
      "outputs": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 269
        },
        "id": "FmM8Z7a7Ml8t",
        "outputId": "56a62eed-0f50-4c58-b682-24ec5a1d6093"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "getPredictionOfActualLabels(cf,2,angle=45)"
      ],
      "outputs": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 273
        },
        "id": "Q-KFHb2UM_Y9",
        "outputId": "b256cf28-ec6f-49cf-b353-6a4baffab2ab"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [],
      "outputs": [],
      "metadata": {
        "id": "fwyn186P8MlN"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "getActualLabelsOfPredictions(cf,0,angle=45)"
      ],
      "outputs": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 269
        },
        "id": "b0G88nNQ8OFH",
        "outputId": "5432080e-a67c-4652-cbe1-23c46fe36493"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "getActualLabelsOfPredictions(cf,1,angle=45)"
      ],
      "outputs": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 269
        },
        "id": "VsAW9p4p8N6z",
        "outputId": "575b1eb2-dd14-4f17-a795-7f9580e2b02e"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "getActualLabelsOfPredictions(cf,2,angle=45)"
      ],
      "outputs": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 275
        },
        "id": "pJMjKsdD8Nq0",
        "outputId": "b6f2fe04-7006-4571-e9ab-6c7ca1492c35"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "import pickle\n",
        "\n",
        "filenamedpcnn = '/content/drive/My Drive/content/BertAttnDPCNN5_colab_attention_False_finance_Exp7DPCNN.pkl'\n",
        "filenamebert = '/content/drive/My Drive/content/BertAttnDPCNN5_colab_attention_False_finance_Exp7BERT.pkl'\n",
        "pickle.dump(bert, open(filenamebert, 'wb'))\n",
        "pickle.dump(dpcnn, open(filenamedpcnn, 'wb'))"
      ],
      "outputs": [],
      "metadata": {
        "id": "Af-V7xQ0NS4M"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "import pickle\n",
        "file = '/content/drive/My Drive/content/BertAttnDPCNN5_colab_attention_False_finance_maplist.pkl'\n",
        "pickle.dump(dpcnn.maps, open(file, 'wb'))"
      ],
      "outputs": [],
      "metadata": {
        "id": "uMrui8fy8lWz"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [],
      "outputs": [],
      "metadata": {
        "id": "J5fMUV06ArM1"
      }
    }
  ]
}